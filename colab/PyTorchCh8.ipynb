{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorchCh8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPk0uwJByUuihDQW91OzVwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e3adf7084264e8b946aa420cbf8c4c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_233c3783f06341da977f25a35398eeee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_403b015704354007b97b54ae3cb7adae",
              "IPY_MODEL_37b39e8103c94a84aa1366471783db20",
              "IPY_MODEL_b44e3bcb20da4775a45024a06eacb5c1"
            ]
          }
        },
        "233c3783f06341da977f25a35398eeee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "403b015704354007b97b54ae3cb7adae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e10e3e0994f54a94bf8e953d2639dfd1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9dc6c22bc474d86b878a1ae638b7ea5"
          }
        },
        "37b39e8103c94a84aa1366471783db20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8dd976be4cbb48a7ae9ea7724a85438f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_527c28f68ea043a292dc54626459fd43"
          }
        },
        "b44e3bcb20da4775a45024a06eacb5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d67f1177ec7143c0a5de0aa0c94aac7b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:06&lt;00:00, 32771186.97it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96870e2eb6b043fe96c3a688b96620ea"
          }
        },
        "e10e3e0994f54a94bf8e953d2639dfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9dc6c22bc474d86b878a1ae638b7ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8dd976be4cbb48a7ae9ea7724a85438f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "527c28f68ea043a292dc54626459fd43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d67f1177ec7143c0a5de0aa0c94aac7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96870e2eb6b043fe96c3a688b96620ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bitdribble/dlwpt-code/blob/master/colab/PyTorchCh8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Telling birds from airplanes: Learning from images using convolutions ([Deep Learning with PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf), Chap. 8)"
      ],
      "metadata": {
        "id": "0SFyRjT09E_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HQOa_p_9Dre",
        "outputId": "b8de50aa-ebcf-437a-f158-a452e6f0b757"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f23a3c2da90>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75) # Simplifies print output of tensors\n",
        "torch.manual_seed(123) # Fixed seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "data_path = '.'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "0e3adf7084264e8b946aa420cbf8c4c8",
            "233c3783f06341da977f25a35398eeee",
            "403b015704354007b97b54ae3cb7adae",
            "37b39e8103c94a84aa1366471783db20",
            "b44e3bcb20da4775a45024a06eacb5c1",
            "e10e3e0994f54a94bf8e953d2639dfd1",
            "f9dc6c22bc474d86b878a1ae638b7ea5",
            "8dd976be4cbb48a7ae9ea7724a85438f",
            "527c28f68ea043a292dc54626459fd43",
            "d67f1177ec7143c0a5de0aa0c94aac7b",
            "96870e2eb6b043fe96c3a688b96620ea"
          ]
        },
        "id": "4zTgNKoQZYU6",
        "outputId": "4484cce9-c6bc-47ab-9bd7-d7f9ddd1ccf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e3adf7084264e8b946aa420cbf8c4c8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "transformed_cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468), # Mean per channel, precomputed in Chap 7\n",
        "                             (0.2470, 0.2435, 0.2616)) # Stddev per channel\n",
        "    ]))\n",
        "transformed_cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "id": "570u2kiRZzcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restrict data from 10 categories to airplanes (category 0) and birds (category 2)\n",
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "\n",
        "cifar2 = [(img, label_map[label]) for img, label in transformed_cifar10 if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label]) for img, label in transformed_cifar10_val if label in [0, 2]]\n"
      ],
      "metadata": {
        "id": "Ui1ogiGNZqcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of linear layers, use 'convolution layers'. The convolution matrix\n",
        "# has weights as below (also has bias, but ignore that for now):\n",
        "#\n",
        "# weight = torch.tensor([[w00, w01, w02],\n",
        "#                        [w10, w11, w12],\n",
        "#                        [w20, w21, w22]])\n",
        "#\n",
        "# This is called the 'kernel'. \n",
        "# Assume a 1-channel, MxN image:\n",
        "#\n",
        "# image = torch.tensor([[i00, i01, i02, i03, ..., i0N],\n",
        "#                       [i10, i11, i12, i13, ..., i1N],\n",
        "#                       [i20, i21, i22, i23, ..., i2N],\n",
        "#                       [i30, i31, i32, i33, ..., i3N],\n",
        "#                       ...\n",
        "#                       [iM0, iM1m iM2, iM3, ..., iMN]])\n",
        "#\n",
        "# We can compute an element of the output image (without bias) as:\n",
        "#\n",
        "# [[o00, o01, ...],\n",
        "#  [o10, o11, ...],\n",
        "#   ...\n",
        "#\n",
        "# where\n",
        "#\n",
        "# o11 = i11 * w00 + i12 * w01 + i13 * w02 + \n",
        "#       i21 * w10 + i22 * w11 + i23 * w12 +\n",
        "#       i31 * w20 + i32 * w21 + i33 * w22\n",
        "#\n",
        "# In essence:\n",
        "# - We 'pad' kernel and image with 0-valued rows and columns\n",
        "# - Steps to obtain oij:\n",
        "#   - Translate the kernel by (i, j), and transpose it\n",
        "#   - Multiply image with translated, transposed kernel, and take sum of elements"
      ],
      "metadata": {
        "id": "L9ruhQkwfkNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1bSJk77_mcn_fytNb_HgYBapXrMyf5vg1)\n",
        "\n",
        "- Just like the elements in the weight matrix of nn.Linear, the weights in the kernel are not known in advance, but they are initialized randomly and updated\n",
        "through backpropagation. \n",
        "\n",
        "- The same kernel, and thus each weight in the kernel, is reused across the whole image. \n",
        "\n",
        "- With a convolution layer, the number of parameters depends not on the number of pixels in the image, as was the case in our fully connected model, but rather on the size of the convolution kernel.\n",
        "\n",
        "Summarizing, by switching to convolutions, we get:\n",
        "- Local operations on a neighborhood\n",
        "- Translation invariance\n",
        "- Models with a lot fewer parameters"
      ],
      "metadata": {
        "id": "-0sZC5b1iHgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# - Image is RGB, with 3 input channels\n",
        "# - Pick arbitrary number of output channels (16). This is needed b/c\n",
        "# of 'lottery ticket hypothesis' (saying that some channels will end up being useless!)\n",
        "# - Kernel size is 3 (shorthand for 3x3). It is square size, but could be rectangular.\n",
        "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
        "conv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VUKBYjFi4uC",
        "outputId": "d93211ea-dedd-4b1d-e28a-27c7ad8154b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.weight.shape, conv.bias.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "catFO78pi4bB",
        "outputId": "e4445d20-ede0-4a61-b6fb-b2a7bec25bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to add the zeroth batch dimension with\n",
        "# unsqueeze if we want to call the conv module with one input image, since nn.Conv2d\n",
        "# expects a B × C × H × W shaped tensor as input\n",
        "img, _ = cifar2[0] # First image in cifar2 dataset\n",
        "output = conv(img.unsqueeze(0)) # Apply convolution\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXH3g8m8pTl8",
        "outputId": "80aa95b0-cf2e-4c2b-dfe8-d70eb1648005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Image before and after conv\n",
        "subplot = plt.subplot(1, 2, 1)\n",
        "subplot.set_title(\"input\")\n",
        "plt.imshow(img.permute(1, 2, 0), cmap='gray') # Convert image from C x H x W to H x W x C\n",
        "subplot = plt.subplot(1, 2, 2)\n",
        "subplot.set_title(\"output\")\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray') # Use detach() to remove computation graph\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "Nss7ENLbpq27",
        "outputId": "c3f649e1-9fb0-492b-b7ef-3b5d7161df58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output size is 30x30 instead of 32x32 b/c \n",
        "# by default pytorch will slide the convolution kernel\n",
        "# only within the image (and the center of the conv kernel\n",
        "# will not reach the end rows/colums)\n",
        "img.size(),output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKeEwcnJrxyC",
        "outputId": "8acbe905-4e84-4428-e848-2547782e55ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# However, PyTorch gives us the possibility of padding the image\n",
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1) # Now with padding and 1 output channel instead of 16\n",
        "\n",
        "output = conv(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtfNXwVMxkyF",
        "outputId": "5cfa8c80-9c38-4139-b685-5885578bed2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1PGZnYrlzMHbcEVmXYhyY6knvlebUv6q1)"
      ],
      "metadata": {
        "id": "qk15R49Li6nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detecting features with convolutions\n",
        "#\n",
        "# Zero out the bias, and set the kernel to equal weights\n",
        "with torch.no_grad():\n",
        "  conv.bias.zero_()\n",
        "with torch.no_grad():\n",
        "  conv.weight.fill_(1.0 / 9.0)\n",
        "\n",
        "output = conv(img.unsqueeze(0))\n",
        "\n",
        "# The filter produces a blurred version of the image. With equal weights, every pixel of \n",
        "# the output is the average of a neighborhood\n",
        "subplot = plt.subplot(1, 2, 1)\n",
        "subplot.set_title(\"input\")\n",
        "plt.imshow(img.permute(1, 2, 0), cmap='gray')\n",
        "subplot = plt.subplot(1, 2, 2)\n",
        "subplot.set_title(\"output\")\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "wompIus2yOxB",
        "outputId": "b39cef43-d708-44d8-9859-9c7d202062fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try new kernel that gives difference between next column and prev column\n",
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
        "with torch.no_grad():\n",
        "  conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
        "                                 [-1.0, 0.0, 1.0],\n",
        "                                 [-1.0, 0.0, 1.0]])\n",
        "  conv.bias.zero_()\n",
        "\n",
        "output = conv(img.unsqueeze(0))\n",
        "\n",
        "# This kernel detects vertical edges\n",
        "subplot = plt.subplot(1, 2, 1)\n",
        "subplot.set_title(\"input\")\n",
        "plt.imshow(img.permute(1, 2, 0), cmap='gray')\n",
        "subplot = plt.subplot(1, 2, 2)\n",
        "subplot.set_title(\"output\")\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "WoXICAzo0Dyc",
        "outputId": "2e5145f8-d222-4087-9a16-17260ab65069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We could build lots more elaborate filters, such as for detecting horizontal or diagonal edges, or cross-\n",
        "# like or checkerboard patterns, where “detecting” means the output has a high magni-\n",
        "# tude. In fact, the job of a computer vision expert has historically been to come up with\n",
        "# the  most  effective  combination  of  filters  so  that  certain  features  are  highlighted  in\n",
        "# images and objects can be recognized.\n",
        "#\n",
        "# With deep learning, we let kernels be estimated from data in whatever way the dis-\n",
        "# crimination is most effective: for instance, in terms of minimizing the negative cross-\n",
        "# entropy loss between the output and the ground truth. From this angle, \n",
        "# the job of a convolutional neural network is to estimate the ker-\n",
        "# nel of a set of filter banks in successive layers that will transform a multichannel image\n",
        "# into  another  multichannel  image,  where  different  channels  correspond  to  different\n",
        "# features (such as one channel for the average, another channel for vertical edges, and\n",
        "# so on). Figure below shows how the training automatically learns the kernels. "
      ],
      "metadata": {
        "id": "nser1bol1J_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=17iDBagvo6-6pt1aMy81Gj7Z4BcduG7I8)"
      ],
      "metadata": {
        "id": "M83E67eb33ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutions achieve locality and translation invariance. Small convolution \n",
        "# kernels are recommended, 3x3 or 5x5. But what about detecting 'global' features?\n",
        "#\n",
        "# Don't want to use large conv kernels, say 32x32, b/c it's inefficient.\n",
        "# \n",
        "# Idea: use downsampling layer. We stack one convolution after the other and \n",
        "# at the same time downsampling the image between successive convolutions.\n",
        "#\n",
        "# Downsampling could be done in different ways. Scaling an image by half is\n",
        "# the equivalent of taking four neighboring pixels as input and producing \n",
        "# one pixel as output. We could:\n",
        "# - Average the four pixels. This average pooling was a common approach early on but\n",
        "#   has fallen out of favor somewhat.\n",
        "# - Take the maximum of the four pixels. This approach, called max pooling, is currently\n",
        "#   the  most  commonly  used  approach,  but  it  has  a  downside  of  discarding  the\n",
        "#   other three-quarters of the data.\n",
        "# - Perform a strided convolution, where only every Nth pixel is calculated. A 3 × 4 convolu-\n",
        "#   tion with stride 2 still incorporates input from all pixels from the previous layer.\n",
        "#   The  literature  shows  promise  for  this  approach,  but  it  has  not  yet  supplanted\n",
        "#   max pooling.\n",
        "#\n",
        "# We use max pooling."
      ],
      "metadata": {
        "id": "jk1PgrVJ4rVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1scpsEZYYIotuu8bFDTsXGTbaaiHmLmw4)"
      ],
      "metadata": {
        "id": "LJo3LyoX6yzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pool = nn.MaxPool2d(2)\n",
        "output = pool(img.unsqueeze(0))\n",
        "\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10o6Q2VD7Fib",
        "outputId": "414e982f-59f6-441b-e2bb-f70ddfdfb821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1Ya2EIe8uvy4n8B-T__-oQCZ-OuUJgZnG)"
      ],
      "metadata": {
        "id": "k3sN26ul75bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "\n",
        "    nn.MaxPool2d(2),\n",
        "    # ... More layers will be added here later\n",
        ")\n",
        "\n",
        "# 3x32x32 -> 16x32x32 # giving the network a  chance  to  generate  16  independent  \n",
        "#                     # features  that  operate  to  (hopefully)  discrimi-\n",
        "#                     # nate low-level features of birds and airplanes.\n",
        "#         -> 16x16x16 # maxpool downsampling from 32x32 to 16x16 images\n",
        "#         ->  8x16x16 # down to 8 channels for 16x16 images\n",
        "#         ->    8x8x8 # downsampling to 8x8 images\n",
        "\n",
        "# Complete the network with fully connected layers\n",
        "model = nn.Sequential(                          # Layer output:\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1), # 16x32x32\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),                            # 16x16x16\n",
        "\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1), # 8x16x16\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),                            # 8x8x8\n",
        "\n",
        "    nn.Flatten(), # Needed to convert 8x8x8 to 8*8*8\n",
        "\n",
        "    nn.Linear(8*8*8, 32), # 32\n",
        "    nn.Tanh(),\n",
        "\n",
        "    nn.Linear(32, 2) # 2 (planes, birds)\n",
        "    )\n",
        "\n",
        "# The size of the linear layer is dependent on the expected size of the output \n",
        "# of MaxPool2d: 8×8×8 = 512. Let’s count the number of parameters for this small model:\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n",
        "sum(numel_list), numel_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0LykoIh8BcQ",
        "outputId": "8d248ecf-0239-4a6a-a313-2352afe9c096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That’s very reasonable for a limited dataset of such small images. In order to increase\n",
        "# the capacity of the model, we could increase the number of output channels for the\n",
        "# convolution layers (that is, the number of features each convolution layer generates),\n",
        "# which would lead the linear layer to increase its size as well."
      ],
      "metadata": {
        "id": "tbHOEnOJ-LMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=114aiMy8s6OC4BVNkP6hWEkW8szfRFMOX)"
      ],
      "metadata": {
        "id": "n8RQLIyG-vxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model forward pass\n",
        "model(img.unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esDOzj4x_N0M",
        "outputId": "a6c66e68-d920-4dfe-81a3-08458e008cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0975, 0.1355]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the network as submodule\n",
        "#\n",
        "# When  we  want  to  build  models  that  do  more  complex  things  than  just  applying \n",
        "# one layer after another, we need to leave nn.Sequential for something that gives us\n",
        "# added flexibility. PyTorch allows us to use any computation in our model by subclass-\n",
        "# ing nn.Module.\n",
        "#\n",
        "# In order to subclass nn.Module, at a minimum we need to define a forward function\n",
        "# that takes the inputs to the module and returns the output. This is where we define our\n",
        "# module’s computation. The name forward here is reminiscent of a distant past, when\n",
        "# modules  needed  to  define  both  the  forward  and  backward  passes. \n",
        "#\n",
        "# With nn.Module, if we use standard torch operations, autograd will take care of the\n",
        "# backward pass automatically; and indeed, an nn.Module never comes with a backward API.\n",
        "#\n",
        "# Typically, our computation will use other modules — which could be premade, like convolutions, or\n",
        "# custom. To include these submodules, we typically define them in the constructor\n",
        "# __init__ and assign them to self for use in the forward function. They will, at the\n",
        "# same time, hold their parameters throughout the lifetime of our module. Note that you\n",
        "# need to call super().__init__() before you can do that (or PyTorch will remind you).\n",
        "#\n",
        "# In the constructor, we  instantiate  all  the  nn.Conv2d, nn.Linear, and so on \n",
        "# that we previously passed to nn.Sequential,\n",
        "# and then use their instances one after another in forward().\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.Tanh()\n",
        "\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.Tanh()\n",
        "\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(8*8*8, 32)\n",
        "    self.act3 = nn.Tanh()\n",
        "\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool1(self.act1(self.conv1(x)))\n",
        "    out = self.pool2(self.act2(self.conv2(out)))\n",
        "    out = out.view(-1, 8*8*8) # In place of nn.Flatten()\n",
        "    out = self.act3(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "# We leave the batch dimension as –1 in the call to view, since we\n",
        "# don’t know how many samples will be in the batch\n",
        "\n",
        "# Architecture comments:\n",
        "#\n",
        "# 1. The size of our intermediate values is generally shrinking. This is done \n",
        "# by reducing the number of channels in the convolutions, by\n",
        "# reducing the number of pixels through pooling, and by having an output dimension\n",
        "# lower than the input dimension in the linear layers. This is a common trait of\n",
        "# classification networks. However, in many popular architectures like the ResNets, \n",
        "# the reduction is achieved by pooling in the spatial resolution, but the number of  \n",
        "# channels increases (still resulting in a reduction in size). It seems that \n",
        "# our pattern of fast information reduction works well with networks of limited depth and \n",
        "# small images; but for deeper networks, the decrease is typically slower.\n",
        "#\n",
        "# 2. The initial layer is special in that it is the only one drastically increasing the output size. \n",
        "# This can be thought as an embedding in a higher dimensional space followed by doing simpler\n",
        "# layer. This is called the 'kernel trick'."
      ],
      "metadata": {
        "id": "vsejWEnIn9Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning an instance of nn.Module to an attribute in an nn.Module, as\n",
        "# we did in the earlier constructor, automatically registers the module as a submodule.\n",
        "# \n",
        "# The submodules must be top-level attributes, not buried inside list or\n",
        "# dict instances! Otherwise the optimizer will not be able to locate the sub-\n",
        "# modules (and, hence, their parameters). For situations where your model\n",
        "# requires a list or dict of submodules, PyTorch provides nn.ModuleList and nn.ModuleDict.\n",
        "#\n",
        "# This allows Net to have access to the parameters of its submodules without further\n",
        "# action by the user, as model.parameters()\n",
        "#\n",
        "# Whether using Sequential() or Module() type modules, \n",
        "# parameters are accessed as model.parameters()\n",
        "model = Net()\n",
        "numel_list = [p.numel() for p in model.parameters() if p.requires_grad == True]\n",
        "sum(numel_list), numel_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LFH_d6vDwkN",
        "outputId": "1ee96b62-2a49-4375-f2e5-9c5a33860064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, 432 = 3 input channels x 9 conv kernel size x 16 out channels for weights,\n",
        "# 16 = number of biases\n",
        "# 1152 = 16 in channels x 9 conv kernel size x 8 out channels,\n",
        "# 8 = number of biases,\n",
        "# 16,384 = 8x8x8 inputs x32 outputs for weights,\n",
        "# 32 = number of biases,\n",
        "# 64 = 32 inputs x2 outputs,\n",
        "# 2 = number of biases"
      ],
      "metadata": {
        "id": "1ieRcd4BGQ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "52X0y89-nKgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# - Feed the inputs through the model (the forward pass).\n",
        "# - Compute the loss (also part of the forward pass).\n",
        "# - Zero any old gradients.\n",
        "# - Call  loss.backward()  to  compute  the  gradients  of  the  loss  with  respect  to  all\n",
        "#   parameters (the backward pass).\n",
        "# - Have the optimizer take a step in toward lower loss.\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, log_epochs=0):\n",
        "  for epoch in range(n_epochs):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader: # Loops over our dataset in the batches the data loader creates for us\n",
        "      outputs = model(imgs) # Feed a batch through the model\n",
        "      loss = loss_fn(outputs, labels) # ...compute the loss we wish to minimize\n",
        "\n",
        "      optimizer.zero_grad() # After getting rid of the gradients from the last round ...\n",
        "      loss.backward() # ... performs the backward step. That is, we compute the gradients of all parameters we \n",
        "                      # want the network to learn.\n",
        "      optimizer.step() # Update the model\n",
        "\n",
        "      loss_train += loss.item() # Sums the losses we saw over all batches in the epoch.\n",
        "                                # We transform the loss to a Python number with .item(), \n",
        "                                # to escape the gradients.\n",
        "\n",
        "    if log_epochs is not 0 and ((epoch+1) % log_epochs == 0 or (epoch+1) == n_epochs):\n",
        "      print(f\"{datetime.datetime.now()} Epoch {epoch+1}, \"\n",
        "            f\"Training loss {loss_train / len(train_loader):.3f}\")\n",
        "      # Training loss divides by the length of the training data loader to get \n",
        "      # the average loss per batch. This is a more intuitive measure to display than the sum.\n",
        "\n",
        "# The DataLoader batches up the examples of our cifar2 dataset. \n",
        "# Shuffling randomizes the order of the examples from the dataset.\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model = Net() # Instantiates our network ...\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2) # ... the stochastic gradient \n",
        "                                                   # descent optimizer we have \n",
        "                                                   # been working with ..\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # ... and the cross entropy loss\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    log_epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMu8wRg6kPP3",
        "outputId": "c6a90fc9-e787-42ee-b8bf-c55c2d915179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-09 14:45:53.417711 Epoch 10, Training loss 0.328\n",
            "2022-01-09 14:46:32.224949 Epoch 20, Training loss 0.289\n",
            "2022-01-09 14:47:11.222132 Epoch 30, Training loss 0.264\n",
            "2022-01-09 14:47:50.444621 Epoch 40, Training loss 0.241\n",
            "2022-01-09 14:48:29.368129 Epoch 50, Training loss 0.226\n",
            "2022-01-09 14:49:08.427316 Epoch 60, Training loss 0.207\n",
            "2022-01-09 14:49:47.412806 Epoch 70, Training loss 0.193\n",
            "2022-01-09 14:50:26.439146 Epoch 80, Training loss 0.182\n",
            "2022-01-09 14:51:05.365394 Epoch 90, Training loss 0.166\n",
            "2022-01-09 14:51:44.446271 Epoch 100, Training loss 0.153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measuring accuracy\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad(): # We do not want gradients here, as we will not want to update the parameters.\n",
        "      for imgs, labels in loader:\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1) # Gives us the index of the highest value as output\n",
        "\n",
        "        total += labels.shape[0] # Counts the number of examples, so total is increased by the batch size\n",
        "\n",
        "        # Comparing the predicted class that had the maximum probability and the ground-truth\n",
        "        # labels, we first get a Boolean array. Taking the sum gives the number of items in the batch\n",
        "        # where the prediction and ground truth agree.\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "    print(f\"Accuracy {name}: {correct / total:.2f}\")\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "176A2mSXR0we",
        "outputId": "dd11d224-8880-42e9-9721-a22a45ed7c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.94\n",
            "Accuracy val: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and loading the model\n",
        "torch.save(model.state_dict(), \"birds_vs_airplanes.pt\")\n",
        "\n",
        "# We will have to make sure we don’t change the definition of Net between saving and \n",
        "# later loading the model state.\n",
        "loaded_model = Net()\n",
        "\n",
        "loaded_model.load_state_dict(torch.load(\"birds_vs_airplanes.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snsmXf9QTRKc",
        "outputId": "133905be-06c8-46d8-9146-085879a7201d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training on the GPU\n",
        "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "print(f\"Training on device {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA_TWuMJTnR8",
        "outputId": "0b584e08-6289-4d1c-c504-ceeb536d5832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Amend the training loop by moving tensors to gpu\n",
        "def training_loop_gpu(n_epochs, optimizer, model, loss_fn, train_loader, log_epochs=0):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device) # Move to gpu\n",
        "      labels = labels.to(device=device) # Move to gpu\n",
        "\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_train += loss.item()\n",
        "\n",
        "    if log_epochs is not 0 and ((epoch+1) % log_epochs == 0 or (epoch+1) == n_epochs):\n",
        "      print(f\"{datetime.datetime.now()} Epoch {epoch+1}, \"\n",
        "            f\"Training loss {loss_train / len(train_loader):.3f}\")\n",
        "\n",
        "def validate_gpu(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad(): \n",
        "      for imgs, labels in loader:\n",
        "        imgs = imgs.to(device=device) # Move to gpu\n",
        "        labels = labels.to(device=device) # Move to gpu\n",
        "\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1) \n",
        "\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "\n",
        "\n",
        "    print(f\"Accuracy {name}: {correct / total:.2f}\")"
      ],
      "metadata": {
        "id": "FQaQBPU_T5H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gpu = Net().to(device=device)\n",
        "optimizer = optim.SGD(model_gpu.parameters(), lr=1e-2)\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "training_loop_gpu(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model_gpu,\n",
        "\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    log_epochs = 10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_WlaZRrVL2w",
        "outputId": "de59f1fd-b84f-4625-e95f-7104e2632201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-09 14:53:13.936882 Epoch 10, Training loss 0.341\n",
            "2022-01-09 14:53:19.766887 Epoch 20, Training loss 0.301\n",
            "2022-01-09 14:53:25.612316 Epoch 30, Training loss 0.277\n",
            "2022-01-09 14:53:31.415174 Epoch 40, Training loss 0.257\n",
            "2022-01-09 14:53:37.264388 Epoch 50, Training loss 0.235\n",
            "2022-01-09 14:53:43.087479 Epoch 60, Training loss 0.222\n",
            "2022-01-09 14:53:48.907270 Epoch 70, Training loss 0.206\n",
            "2022-01-09 14:53:54.754319 Epoch 80, Training loss 0.193\n",
            "2022-01-09 14:54:00.578399 Epoch 90, Training loss 0.181\n",
            "2022-01-09 14:54:06.423398 Epoch 100, Training loss 0.168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
        "\n",
        "validate_gpu(model_gpu, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he11z2mcWp7O",
        "outputId": "655587b9-78ce-4040-bd1f-920d7e3ae093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy train: 0.93\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    }
  ]
}